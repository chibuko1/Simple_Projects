{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6703aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chibu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chibu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk\n",
    " \n",
    "#nltk.download('stopwords')\n",
    " \n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dd91684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author: Chibuko Ozobu\n",
    "#Date: 4/29/22\n",
    "#Description:    Convert a text file into a string.\n",
    "#                Split a string into words, excluding punctuation marks.\n",
    "#                Remove stop words from the string.\n",
    "#                Lemmatize the words in the string so that all words are stem words.\n",
    "#                Count the frequency of each stem word and store the results in a dictionary.\n",
    "#                Convert the dictionary to a JSON file.\n",
    "\n",
    "import json\n",
    "import nltk \n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#txt file to string\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, \"r\", encoding ='utf-8') as f:\n",
    "        movie_file = f.read()\n",
    "    return movie_file\n",
    "\n",
    "#split string into words\n",
    "def split_text(text):\n",
    "    txt = text.lower()\n",
    "    txt = txt.replace(\"\\n\", \" \") #replace all newlines with whitespace\n",
    "    word_list = txt.split(\" \")\n",
    "    for word in word_list:\n",
    "        if word == \"\":\n",
    "            word_list.remove(\"\")\n",
    "    return word_list\n",
    "    \n",
    "#removing stop words from string\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(\"\")# if extra whitespace somehow made its way into the list of words from the file\n",
    "def remove_stop_words(words,stop_words):\n",
    "    punc = list(string.punctuation)\n",
    "    final_words = []\n",
    "    for w in words:\n",
    "     if w in stop_words:\n",
    "            words.remove(w)\n",
    "    for word in words:\n",
    "        for p in punc:\n",
    "            if (p == word[len(word)-1] and len(word) > 1): #remove endline punctuation\n",
    "                word = word[0:len(word)-1]\n",
    "            elif (p == word[0] and len(word) > 1): #remove frontline punctuation\n",
    "                word = word[1:len(word)]    \n",
    "        final_words.append(word)\n",
    "    return final_words\n",
    " \n",
    "#lemmatizing words\n",
    "def lemmatize_words(words_clean): \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    l_words = []\n",
    "    for w in words_clean:\n",
    "        word_lemmatized =  lemmatizer.lemmatize(w)\n",
    "        l_words.append(word_lemmatized)\n",
    "    return l_words\n",
    "\n",
    "#count words\n",
    "def compute_frequency_words(words_lemmatized):\n",
    "    word_freq = dict()\n",
    "    for word in words_lemmatized:\n",
    "        if(word in word_freq): #if the word already exists in the dictionary\n",
    "            # increment key's value\n",
    "            word_freq[word] = word_freq[word] + 1\n",
    "        else:\n",
    "            #add key to dict w/ value of 1\n",
    "            word_freq.update({word:1})\n",
    "    return word_freq\n",
    "\n",
    "#convert to JSON file\n",
    "def save_words_frequency(words_frequency,file_path=\"chibuko_words_frequency.json\"):\n",
    "    with open(file_path, \"w\") as outfile:\n",
    "        json.dump(words_frequency, outfile)\n",
    " \n",
    "\n",
    "text = read_text_file(\"frankenstein_letter1\")\n",
    "words = split_text(text)\n",
    "words_clean = remove_stop_words(words,stop_words)\n",
    "words_lemmatized = lemmatize_words(words_clean)\n",
    "words_frequency = compute_frequency_words(words_lemmatized)\n",
    "save_words_frequency(words_frequency,file_path=\"chibuko_words_frequency.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254b7e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
